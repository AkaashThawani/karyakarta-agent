# Fix Target - What Do We Want?

**Date**: 2025-11-13  
**Purpose**: Define clear goals before implementing any fixes

---

## üéØ PRIMARY GOAL

**Enable follow-up questions to work correctly.**

### Current Behavior ‚ùå
```
User: "Search for flights from NYC to Chicago"
Agent: [Returns flight data]

User: "Show that as a table"  
Agent: "I don't have any data to show" ‚ùå
```

### Desired Behavior ‚úÖ
```
User: "Search for flights from NYC to Chicago"
Agent: [Returns flight data + stores in state]

User: "Show that as a table"
Agent: [Accesses previous data from state]
Agent: [Formats as markdown table] ‚úÖ
```

---

## üîç SPECIFIC ISSUES TO FIX

### Issue #1: Conversation History Not Used
**What's broken**: MultiAgentManager loads conversation_history but never passes it to LLM calls

**Where**: 
- `src/agents/reason_agent.py` - execute() doesn't pass context
- `src/agents/task_analyzer.py` - analyze_task() doesn't accept history
- `src/agents/execution_engine.py` - execute_plan() doesn't use history
- `src/agents/result_processor.py` - synthesize_results() doesn't see history

**Success criteria**:
- [ ] All LLM prompts include previous 3-5 messages as context
- [ ] Agent can reference "it", "that", "them" from previous responses
- [ ] Follow-up questions about previous data work

### Issue #2: Accumulated Data Lost Between Requests
**What's broken**: ExecutionContext.accumulated_data is temporary, disappears after task

**Where**: `src/agents/execution_engine.py` - ExecutionContext is recreated each time

**Success criteria**:
- [ ] Data from step 1 available in step 2 (within same task) ‚úÖ Already works
- [ ] Data from task 1 available in task 2 (follow-up questions) ‚ùå Currently broken
- [ ] Multi-turn conversations maintain data context

### Issue #3: Previous Results Not Propagated
**What's broken**: previous_results loaded but not passed to components

**Where**: All agent components don't receive previous_results

**Success criteria**:
- [ ] Agent knows what data it previously extracted
- [ ] Can format/transform previous data without re-extracting
- [ ] Can answer "how many" questions about previous results

---

## ‚úÖ MUST HAVES (Non-Negotiable)

### 1. Conversation Continuity
- [ ] **Follow-up questions work** - Primary requirement
- [ ] Agent remembers previous 5 messages minimum
- [ ] Can reference previous data without re-querying

### 2. Multi-Step Task Reliability  
- [ ] **Data flows between steps** - Already works, don't break
- [ ] Parameter resolution works (DataFlowResolver)
- [ ] Validation/replanning works (already implemented)

### 3. No Breaking Changes
- [ ] **USE_MULTI_AGENT_SYSTEM = True still works**
- [ ] All existing tools continue to function
- [ ] API contract unchanged
- [ ] No frontend changes required

---

## üéÅ NICE TO HAVES (If Easy)

### 1. Unified State Management
- [ ] Both Path A and Path B use same state system
- [ ] MessagesState integrated into MultiAgentManager
- [ ] Proper LangGraph checkpointing

### 2. Performance Optimization
- [ ] Reduce unnecessary LLM calls
- [ ] Cache frequently used data
- [ ] Smart context window management

### 3. Better Error Recovery
- [ ] Graceful handling when history unavailable
- [ ] Fallback to stateless mode if needed

---

## üö´ EXPLICITLY DON'T WANT

### 1. Don't Change Core Architecture
- ‚ùå **NO** replacing multi-agent system
- ‚ùå **NO** removing ReasonAgent/ExecutorAgent
- ‚ùå **NO** switching back to Path A permanently
- ‚úÖ Keep sophisticated planning capabilities

### 2. Don't Break Existing Functionality
- ‚ùå **NO** breaking Playwright automation
- ‚ùå **NO** breaking task decomposition
- ‚ùå **NO** breaking adaptive execution
- ‚úÖ Everything that works today should still work

### 3. Don't Add Complexity
- ‚ùå **NO** adding new databases
- ‚ùå **NO** adding new services
- ‚ùå **NO** major refactoring
- ‚úÖ Minimal changes to achieve goal

---

## üìä SUCCESS METRICS

### Functional Tests
```python
# Test 1: Follow-up Question
user_msg_1 = "Find 5 restaurants in Seattle"
agent_response_1 = agent.execute(user_msg_1)
# Expected: Returns 5 restaurants

user_msg_2 = "Show that as a table"
agent_response_2 = agent.execute(user_msg_2)
# Expected: ‚úÖ Formats previous restaurants as table
# Current:  ‚ùå Says "I don't have data"
```

```python
# Test 2: Reference Previous Data
user_msg_1 = "Search flights NYC to LA"
agent_response_1 = agent.execute(user_msg_1)

user_msg_2 = "How many results did you find?"
agent_response_2 = agent.execute(user_msg_2)
# Expected: ‚úÖ "I found 7 flights"
# Current:  ‚ùå "I don't have that information"
```

```python
# Test 3: Transform Previous Data
user_msg_1 = "Get product prices from Amazon"
agent_response_1 = agent.execute(user_msg_1)

user_msg_2 = "Sort them by price"
agent_response_2 = agent.execute(user_msg_2)
# Expected: ‚úÖ Sorts previous results
# Current:  ‚ùå Doesn't have previous data
```

### Technical Validation
- [ ] conversation_history passed to all agent components
- [ ] LLM prompts include history in all calls
- [ ] previous_results accessible in result processing
- [ ] No duplicate data fetching for follow-ups

---

## üéØ ACCEPTANCE CRITERIA

**The fix is COMPLETE when:**

1. ‚úÖ User can ask "show that as a table" and agent formats previous data
2. ‚úÖ User can ask "how many?" and agent answers from previous results  
3. ‚úÖ User can ask to filter/sort previous data without re-fetching
4. ‚úÖ Existing multi-step tasks still work (don't break DataFlowResolver)
5. ‚úÖ All current tools still function
6. ‚úÖ No changes required to frontend

**Bonus points if:**
- Minimal code changes (prefer parameter passing over refactoring)
- No new dependencies
- No performance degradation

---

## üöÄ IMPLEMENTATION CONSTRAINTS

### Time Budget
- **Maximum**: 4 hours of coding
- **Target**: 2 hours (minimal fix)
- If taking longer ‚Üí rethink approach

### Code Changes Budget  
- **Maximum**: 10 files modified
- **Target**: 6 files (ReasonAgent, TaskAnalyzer, ExecutionEngine, ResultProcessor, 2 others)
- Prefer small targeted changes over large refactors

### Testing Budget
- **Minimum**: 3 manual tests (follow-up question scenarios)
- **Target**: Add automated tests for conversation continuity
- Don't ship without testing follow-up questions

---

## üõ†Ô∏è PROPOSED APPROACH

### Option 1: Pass Context Through Chain (Recommended)
**Time**: ~90 minutes  
**Risk**: Low  
**Benefit**: Immediate fix

**Changes**:
1. ReasonAgent.execute() ‚Üí Pass context to all sub-components
2. TaskAnalyzer.analyze_task() ‚Üí Accept and use conversation_history  
3. ExecutionEngine.execute_plan() ‚Üí Accept and use history
4. ResultProcessor.synthesize_results() ‚Üí Accept and use history
5. Update all LLM prompts to include history

**Pros**:
- ‚úÖ Quick to implement
- ‚úÖ Low risk
- ‚úÖ Works with current architecture

**Cons**:
- ‚ö†Ô∏è Not using MessagesState (proper solution)
- ‚ö†Ô∏è Manual history management

### Option 2: Integrate MessagesState (Proper Solution)
**Time**: ~8 hours  
**Risk**: Medium  
**Benefit**: Long-term clean architecture

**Changes**:
1. Create multi_agent_graph.py with LangGraph nodes
2. Convert MultiAgentManager to use MessagesState
3. All agents work with messages instead of dicts
4. Proper checkpointing integration

**Pros**:
- ‚úÖ Proper LangGraph integration
- ‚úÖ Automatic history management
- ‚úÖ Better long-term architecture

**Cons**:
- ‚ö†Ô∏è Takes longer
- ‚ö†Ô∏è More complex changes
- ‚ö†Ô∏è Higher risk of breaking things

---

## üí° RECOMMENDATION

**Start with Option 1** (Pass Context Through Chain)

**Why**:
- Gets us to working state quickly
- Low risk of breaking existing functionality
- Validates that this solves the problem
- Can do Option 2 later as proper refactor

**Then** (if time permits):
- Option 2 as "Phase 2" improvement
- Proper MessagesState integration
- Clean architecture

---

## üìù QUESTIONS TO ANSWER BEFORE CODING

1. **Do we all agree follow-up questions are the primary goal?** ‚Üí Yes/No
2. **Are we OK with Option 1 (band-aid) first?** ‚Üí Yes/No  
3. **What's the absolute minimum fix we'd accept?** ‚Üí Just follow-up questions working
4. **What existing functionality CANNOT break?** ‚Üí Multi-step tasks, tools, Playwright
5. **How will we test this works?** ‚Üí Manual follow-up question tests

---

## ‚úÖ SIGN-OFF

Before implementing ANY code changes:

- [ ] Team agrees on primary goal (follow-up questions)
- [ ] Team agrees on approach (Option 1 vs Option 2)
- [ ] Team agrees on acceptance criteria
- [ ] Team agrees on what NOT to change
- [ ] Team reviewed this document

**Approved by**: _________________  
**Date**: _________________

---

**Now, what do we want to do?**
- A) Implement Option 1 (90 min quick fix)
- B) Implement Option 2 (8 hour proper solution)
- C) Revise the fix target (change goals/scope)
